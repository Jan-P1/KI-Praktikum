{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest Cover-Type\n",
    "Optuna example that optimizes a neural network classifier configuration for the breast cancer dataset using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import more useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# for neural networks\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# machine learning basics\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "# train_test_split was moved from cross_validation to model_selection in 0.18\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 1000\n",
    "# number of epochs\n",
    "EPOCHS = 100\n",
    "\n",
    "# needed to save best model so far\n",
    "global best_accuracy_so_far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The objective function for optuna to optimize the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    global best_accuracy_so_far\n",
    "\n",
    "    # Clear clutter from previous Keras session graphs.\n",
    "    clear_session()\n",
    "\n",
    "    trees = pd.read_csv('../train.csv')\n",
    "\n",
    "    # create features X and labels y (we need later to train model)\n",
    "    # we do not want to have the first column in X (this is the Id) and also not the last column (this is the label)\n",
    "    X = trees.values[:,1:-1]\n",
    "    y = trees['Cover_Type'].values\n",
    "    \n",
    "    # print(y.shape())\n",
    "\n",
    "    # split dataset into training and validation datasets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    # Fit only to the training data\n",
    "    scaler.fit(X_train)\n",
    "    # save fitted scaler, because you need it later for the test dataset\n",
    "    pickle.dump(scaler, open(\"scaler.p\", \"wb\"))\n",
    "\n",
    "    # Now apply the transformations to the data:\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    # create neural network\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Input(shape=54))\n",
    "    model.add(Dense(16, kernel_initializer='uniform', activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(units=trial.suggest_int(\"units\", 8, 24, step=4), kernel_initializer='uniform',\n",
    "                    activation=trial.suggest_categorical(\"activation\", [\"relu\", \"linear\"])))\n",
    "    # Adding dropout to prevent overfitting\n",
    "    model.add(Dropout(rate=trial.suggest_float(\"rate\", 0.0, 0.1, step=0.1)))\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\n",
    "    # We compile our model with a sampled learning rate.\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    # train neural network\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        shuffle=True,\n",
    "        batch_size=BATCHSIZE,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # Evaluate the model accuracy on the validation set.\n",
    "    score = model.evaluate(X_val, y_val, verbose=0)\n",
    "\n",
    "    # save best model so far to be able to use the best model later to predict with test data\n",
    "    if score[1] >= best_accuracy_so_far:\n",
    "        tf.keras.models.save_model(model, '{0}.mdl'.format(trial.number))\n",
    "        best_accuracy_so_far = score[1]\n",
    "\n",
    "    # return accuracy\n",
    "    return score[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function\n",
    "Use optuna to do hyperparameter optimization to find optimal neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy_so_far = -100\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "# be cautious with the number of trials: Do not use a number larger than 50\n",
    "# this call starts the hyperparameter optimization process: the above define function \"objective\" is called with\n",
    "# n_trials different hyperparameter combinations\n",
    "study.optimize(objective, n_trials=5, timeout=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse the best model and use it to predict accuracy on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(trial)\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best model. This model was saved in the function \"objective\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tf.keras.models.load_model('{0}.mdl'.format(trial.number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return performance of final model on new data (test data)\n",
    "TODO: only load test data here, that you get a few days before the deadline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = pd.read_csv('../train.csv')\n",
    "\n",
    "# create features X and labels y (we need later to train model)\n",
    "# we do not want to have the first column in X (this is the Id) and also not the last column (this is the label)\n",
    "X_test = trees.values[:,1:-1]\n",
    "y_test = trees['Cover_Type'].values\n",
    "\n",
    "scaler = pickle.load(open(\"scaler.p\", \"rb\"))\n",
    "# important: preprocessing of test dataset has to be the same as for the training dataset\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "print(y_pred)\n",
    "# create labels out of predictions\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_labels)\n",
    "\n",
    "print(\"Our accuracy is {}%\".format(((cm[0][0] + cm[1][1]) / cm.sum()) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cm, annot=True)\n",
    "plt.savefig('confmat.png')\n",
    "\n",
    "print(classification_report(y_test, y_pred_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e0ca292f27c3eddb7e3edb2bf1007a82d9b90e306ea31bedddd394b01cba79e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
