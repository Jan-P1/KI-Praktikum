{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# for neural networks\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# train_test_split was moved from cross_validation to model_selection in 0.18\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import MinMixScaler for normalization of training data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Define parameters batchsize and epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BATCHSIZE = 1000\n",
    "# number of epochs\n",
    "EPOCHS = 300\n",
    "\n",
    "# needed to save best model so far\n",
    "global best_accuracy_so_far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## The objective function for optuna to optimize the hyperparameters as well as the preperation of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    global best_accuracy_so_far\n",
    "\n",
    "    # Clear clutter from previous Keras session graphs.\n",
    "    clear_session()\n",
    "\n",
    "    # Import DataFrame\n",
    "    trees = pd.read_csv('../train.csv', header=0, index_col=\"Id\")\n",
    "\n",
    "    # Convert Wilderness_Types to single column\n",
    "    columns = ['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4']\n",
    "    wilderness_areas = []\n",
    "    for index, row in trees.iterrows():\n",
    "        dummy = 'N/A'\n",
    "        i = 0\n",
    "        for col in columns:\n",
    "            i += 1\n",
    "            if row[col] == 1:\n",
    "                dummy = i\n",
    "                break\n",
    "        wilderness_areas.append(dummy)\n",
    "    trees['Wilderness_Areas'] = wilderness_areas\n",
    "\n",
    "\n",
    "\n",
    "    # Convert Soil_Types to single column\n",
    "    # Drop Soil_Types that never occur (always 0) here and later in actual drop\n",
    "    columns = [\n",
    "        'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4',\n",
    "        'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8',\n",
    "        'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n",
    "        'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16',\n",
    "        'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n",
    "        'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n",
    "        'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n",
    "        'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n",
    "        'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n",
    "        'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\n",
    "    soil_types = []\n",
    "    for index, row in trees.iterrows():\n",
    "        dummy = 'N/A'\n",
    "        i = 0\n",
    "        for col in columns:\n",
    "            i += 1\n",
    "            if row[col] == 1:\n",
    "                dummy = i\n",
    "                break\n",
    "        soil_types.append(dummy)\n",
    "    trees['Soil_Types'] = soil_types\n",
    "\n",
    "    trees.drop(columns=['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4'], inplace=True)\n",
    "\n",
    "    trees.drop(columns=[\n",
    "        'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4',\n",
    "        'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8',\n",
    "        'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n",
    "        'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16',\n",
    "        'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n",
    "        'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n",
    "        'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n",
    "        'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n",
    "        'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n",
    "        'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    # get features X and labels y\n",
    "    X = trees.values[:,:-1]\n",
    "    y = trees[\"Cover_Type\"]-1\n",
    "    # split dataset into training and validation datasets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    # Fit only to the training data\n",
    "    scaler.fit(X_train)\n",
    "    # save fitted scaler, because you need it later for the test dataset\n",
    "    pickle.dump(scaler, open(\"scaler.p\", \"wb\"))\n",
    "\n",
    "    # Now apply the transformations to the data:\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    # create neural network\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Input(shape=12))\n",
    "    model.add(Dense(128, kernel_initializer='random_uniform', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(256, kernel_initializer='he_uniform', activation='selu'))\n",
    "    model.add(Dense(units=trial.suggest_int(\"units\", 64, 128, step=16), kernel_initializer='he_uniform',\n",
    "                    activation=trial.suggest_categorical(\"activation1\", [\"selu\", \"relu\", \"linear\"])))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(units=trial.suggest_int(\"units\", 64, 128, step=16), kernel_initializer='uniform',\n",
    "                   activation=trial.suggest_categorical(\"activation2\", [\"relu\", \"linear\"])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(tf.keras.layers.LayerNormalization(axis=1 , center=True , scale=True))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    # Adding dropout to prevent overfitting\n",
    "    model.add(Dropout(rate=trial.suggest_float(\"rate\", 0.2, 0.5, step=0.1)))\n",
    "    model.add(Dense(7, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    # Sigmoid outputs the probability predicted for each label individually\n",
    "\n",
    "\n",
    "    # We compile our model with a sampled learning rate.\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    my_callbacks = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "\n",
    "    # train neural network\n",
    "    my_model = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        shuffle=True,\n",
    "        batch_size=BATCHSIZE,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=False,\n",
    "        callbacks=my_callbacks\n",
    "    )\n",
    "\n",
    "\n",
    "    # Evaluate the model accuracy on the validation set.\n",
    "    score = model.evaluate(X_val, y_val, verbose=0)\n",
    "\n",
    "    # save best model so far to be able to use the best model later to predict with test data\n",
    "    if score[1] >= best_accuracy_so_far:\n",
    "        tf.keras.models.save_model(model, '{0}.mdl'.format(trial.number))\n",
    "        best_accuracy_so_far = score[1]\n",
    "\n",
    "    # return accuracy\n",
    "    return score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_accuracy_so_far = -100\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "# be cautious with the number of trials: Do not use a number larger than 50\n",
    "# this call starts the hyperparameter optimization process: the above define function \"objective\" is called with\n",
    "# n_trials different hyperparameter combinations\n",
    "study.optimize(objective, n_trials=5, timeout=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Analyse the best model and use it to predict accuracy on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(trial)\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Load the best model. This model was saved in the function \"objective\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_model = tf.keras.models.load_model('{0}.mdl'.format(trial.number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Return performance of final model on new data (test data)\n",
    "TODO: only load test data here, that you get a few days before the deadline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trees = pd.read_csv('../test_file.csv', header=0, index_col=\"Id\")\n",
    "\n",
    "# Do the same as with the training data at the beginning\n",
    "columns = ['Wilderness_Area_0', 'Wilderness_Area_1', 'Wilderness_Area_2', 'Wilderness_Area_3']\n",
    "soil_types = []\n",
    "wilderness_areas = []\n",
    "for index, row in trees.iterrows():\n",
    "    dummy = 'N/A'\n",
    "    i = 0\n",
    "    for col in columns:\n",
    "        i += 1\n",
    "        if row[col] == 1:\n",
    "            dummy = i\n",
    "            break\n",
    "    wilderness_areas.append(dummy)\n",
    "trees['Wilderness_Areas'] = wilderness_areas\n",
    "\n",
    "columns = [\n",
    "    'Soil_Type_0', 'Soil_Type_1', 'Soil_Type_2', 'Soil_Type_3',\n",
    "    'Soil_Type_4', 'Soil_Type_5', 'Soil_Type_6', 'Soil_Type_7',\n",
    "    'Soil_Type_8', 'Soil_Type_9', 'Soil_Type_10', 'Soil_Type_11',\n",
    "    'Soil_Type_12', 'Soil_Type_13', 'Soil_Type_14', 'Soil_Type_15',\n",
    "    'Soil_Type_16', 'Soil_Type_17', 'Soil_Type_18', 'Soil_Type_19',\n",
    "    'Soil_Type_20', 'Soil_Type_21', 'Soil_Type_22', 'Soil_Type_23',\n",
    "    'Soil_Type_24', 'Soil_Type_25', 'Soil_Type_26', 'Soil_Type_27',\n",
    "    'Soil_Type_28', 'Soil_Type_29', 'Soil_Type_30', 'Soil_Type_31',\n",
    "    'Soil_Type_32', 'Soil_Type_33', 'Soil_Type_34', 'Soil_Type_35',\n",
    "    'Soil_Type_36', 'Soil_Type_37', 'Soil_Type_38', 'Soil_Type_39']\n",
    "soil_types = []\n",
    "\n",
    "for index, row in trees.iterrows():\n",
    "    dummy = 'N/A'\n",
    "    i = 0\n",
    "    for col in columns:\n",
    "        i += 1\n",
    "        if row[col] == 1:\n",
    "            dummy = i\n",
    "            break\n",
    "    soil_types.append(dummy)\n",
    "trees['Soil_Types'] = soil_types\n",
    "\n",
    "\n",
    "trees.drop(columns=['Wilderness_Area_0', 'Wilderness_Area_1', 'Wilderness_Area_2', 'Wilderness_Area_3'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "trees.drop(columns=[\n",
    "    'Soil_Type_0', 'Soil_Type_1', 'Soil_Type_2', 'Soil_Type_3',\n",
    "    'Soil_Type_4', 'Soil_Type_5', 'Soil_Type_6', 'Soil_Type_7',\n",
    "    'Soil_Type_8', 'Soil_Type_9', 'Soil_Type_10', 'Soil_Type_11',\n",
    "    'Soil_Type_12', 'Soil_Type_13', 'Soil_Type_15', 'Soil_Type_15',\n",
    "    'Soil_Type_16', 'Soil_Type_17', 'Soil_Type_18', 'Soil_Type_19',\n",
    "    'Soil_Type_20', 'Soil_Type_21', 'Soil_Type_22', 'Soil_Type_23',\n",
    "    'Soil_Type_24', 'Soil_Type_25', 'Soil_Type_26', 'Soil_Type_27',\n",
    "    'Soil_Type_28', 'Soil_Type_29', 'Soil_Type_30', 'Soil_Type_31',\n",
    "    'Soil_Type_32', 'Soil_Type_33', 'Soil_Type_34', 'Soil_Type_35',\n",
    "    'Soil_Type_36', 'Soil_Type_37', 'Soil_Type_38', 'Soil_Type_39'], inplace=True)\n",
    "\n",
    "\n",
    "X_test = trees.values[:,1:-1]\n",
    "y_test = trees[\"Cover_Type\"]-1\n",
    "\n",
    "scaler = pickle.load(open(\"scaler.p\", \"rb\"))\n",
    "# important: preprocessing of test dataset has to be the same as for the training dataset\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "print(y_pred)\n",
    "# create labels out of predictions\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_labels)\n",
    "\n",
    "print(\"Our accuracy is {}%\".format(((cm[0][0] + cm[1][1]) / cm.sum()) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Plot heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.heatmap(cm, annot=True)\n",
    "plt.savefig('confmat.png')\n",
    "\n",
    "print(classification_report(y_test, y_pred_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
